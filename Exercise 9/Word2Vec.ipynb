{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# word2vec\n",
    "\n",
    "Ten notatnik ma na celu przedstawienie sposobu tworzenia i wykorzystania reprezentacji werktorowych na przykładzie algorytmu word2vec. W trakcie zadania najpierw stworzymy prostą reprezentację wektorową, a następnie spróbujemy wczytać gotowy model nauczony na dużym korpusie tekstowym.\n",
    "\n",
    "Po wykonaniu tego zadania powinieneś:\n",
    "+ wiedzieć na czym polega word2vec,\n",
    "+ potrafić stworzyć word2vec na własnych danych,\n",
    "+ potrafić wykorzystać word2vec do:\n",
    "\t+ znalezienia podobnych słów,\n",
    "\t+ wyszukiwania słów na zasadzie \"reguły trzech\", \n",
    "\t+ wykrywania niepasujących słów,\n",
    "\t+ do tworzenia wektora cech nadającego się do klasyfikacji,\n",
    "+ wczytać i wykorzystać gotowy model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prosty model\n",
    "\n",
    "Najpierw wczytamy odpowiednie biblioteki i stworzymy mały zbiór treningowy na podstawie znanej piosenki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, logging, re, nltk\n",
    "import pandas as pd\n",
    "\n",
    "RE_SPACES = re.compile(\"\\s+\")\n",
    "RE_HASHTAG = re.compile(\"[@#][_a-z0-9]+\")\n",
    "RE_EMOTICONS = re.compile(\"(:-?\\))|(:p)|(:d+)|(:-?\\()|(:/)|(;-?\\))|(<3)|(=\\))|(\\)-?:)|(:'\\()|(8\\))\")\n",
    "RE_HTTP = re.compile(\"http(s)?://[/\\.a-z0-9]+\")\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "song = \"\"\"Gdzie strumyk płynie z wolna,\n",
    "Rozsiewa zioła maj,\n",
    "Stokrotka rosła polna,\n",
    "A nad nią szumiał gaj,\n",
    "Stokrotka rosła polna,\n",
    "A nad nią szumiał gaj,\n",
    "Zielony gaj.\n",
    "\n",
    "W tym gaju tak ponuro,\n",
    "Że aż przeraża mnie,\n",
    "Ptaszęta za wysoko,\n",
    "A mnie samotnej źle,\n",
    "Ptaszęta za wysoko,\n",
    "A mnie samotnej źle,\n",
    "samotnej źle.\n",
    "\n",
    "Wtem harcerz idzie z wolna.\n",
    "„Stokrotko, witam cię,\n",
    "Twój urok mnie zachwyca,\n",
    "Czy chcesz być mą, czy nie?”\n",
    "\"Twój urok mnie zachwyca,\n",
    "Czy chcesz być mą, czy nie?\n",
    "Czy nie, czy nie?\n",
    "\n",
    "Stokrotka się zgodziła\n",
    "I poszli w ciemny las,\n",
    "A harcerz taki gapa\n",
    "że aż w pokrzywy wlazł,\n",
    "A harcerz taki gapa\n",
    "że aż w pokrzywy wlazł,\n",
    "w pokrzywy wlazł.\n",
    "\n",
    "A ona, ona, ona,\n",
    "Cóż biedna robić ma,\n",
    "Nad gapą pochylona\n",
    "I śmieje się: ha, ha,\n",
    "Nad gapą pochylona\n",
    "I śmieje: się ha, ha,\n",
    "ha, ha, ha, ha.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 1: Podziel piosenkę na wersy, a wersy tokenizuj spacjami. W efekcie powinieneś stworzyć listę list i przypisać ją do zmiennej `sentences`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Gdzie', 'strumyk', 'płynie', 'z', 'wolna,'], ['Rozsiewa', 'zioła', 'maj,'], ['Stokrotka', 'rosła', 'polna,'], ['A', 'nad', 'nią', 'szumiał', 'gaj,'], ['Stokrotka', 'rosła', 'polna,'], ['A', 'nad', 'nią', 'szumiał', 'gaj,'], ['Zielony', 'gaj.'], [], ['W', 'tym', 'gaju', 'tak', 'ponuro,'], ['Że', 'aż', 'przeraża', 'mnie,'], ['Ptaszęta', 'za', 'wysoko,'], ['A', 'mnie', 'samotnej', 'źle,'], ['Ptaszęta', 'za', 'wysoko,'], ['A', 'mnie', 'samotnej', 'źle,'], ['samotnej', 'źle.'], [], ['Wtem', 'harcerz', 'idzie', 'z', 'wolna.'], ['„Stokrotko,', 'witam', 'cię,'], ['Twój', 'urok', 'mnie', 'zachwyca,'], ['Czy', 'chcesz', 'być', 'mą,', 'czy', 'nie?”'], ['\"Twój', 'urok', 'mnie', 'zachwyca,'], ['Czy', 'chcesz', 'być', 'mą,', 'czy', 'nie?'], ['Czy', 'nie,', 'czy', 'nie?'], [], ['Stokrotka', 'się', 'zgodziła'], ['I', 'poszli', 'w', 'ciemny', 'las,'], ['A', 'harcerz', 'taki', 'gapa'], ['że', 'aż', 'w', 'pokrzywy', 'wlazł,'], ['A', 'harcerz', 'taki', 'gapa'], ['że', 'aż', 'w', 'pokrzywy', 'wlazł,'], ['w', 'pokrzywy', 'wlazł.'], [], ['A', 'ona,', 'ona,', 'ona,'], ['Cóż', 'biedna', 'robić', 'ma,'], ['Nad', 'gapą', 'pochylona'], ['I', 'śmieje', 'się:', 'ha,', 'ha,'], ['Nad', 'gapą', 'pochylona'], ['I', 'śmieje:', 'się', 'ha,', 'ha,'], ['ha,', 'ha,', 'ha,', 'ha.']]\n"
     ]
    }
   ],
   "source": [
    "sentences = [row.split() for row in song.split(\"\\n\")]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mając tekst podzielony na zdania a zdania na tokeny, możemy nauczyć model word2vec.\n",
    "\n",
    "**Zad. 2: Naucz model word2vec. [Sprawdź](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec) za co odpowiedzialne są parametry `min_count` i `iter`. Jakie inne parametry mogą być przydatne?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-21 19:40:56,647 : INFO : collecting all words and their counts\n",
      "2021-01-21 19:40:56,648 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-01-21 19:40:56,648 : INFO : collected 79 word types from a corpus of 140 raw words and 39 sentences\n",
      "2021-01-21 19:40:56,648 : INFO : Loading a fresh vocabulary\n",
      "2021-01-21 19:40:56,649 : INFO : effective_min_count=1 retains 79 unique words (100% of original 79, drops 0)\n",
      "2021-01-21 19:40:56,649 : INFO : effective_min_count=1 leaves 140 word corpus (100% of original 140, drops 0)\n",
      "2021-01-21 19:40:56,650 : INFO : deleting the raw counts dictionary of 79 items\n",
      "2021-01-21 19:40:56,651 : INFO : sample=0.001 downsamples 79 most-common words\n",
      "2021-01-21 19:40:56,651 : INFO : downsampling leaves estimated 48 word corpus (35.0% of prior 140)\n",
      "2021-01-21 19:40:56,653 : INFO : estimated required memory for 79 words and 100 dimensions: 102700 bytes\n",
      "2021-01-21 19:40:56,654 : INFO : resetting layer weights\n",
      "2021-01-21 19:40:56,673 : INFO : training model with 3 workers on 79 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-01-21 19:40:56,677 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:40:56,678 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:40:56,678 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:40:56,679 : INFO : EPOCH - 1 : training on 140 raw words (53 effective words) took 0.0s, 23635 effective words/s\n",
      "2021-01-21 19:40:56,681 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:40:56,682 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:40:56,682 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:40:56,683 : INFO : EPOCH - 2 : training on 140 raw words (46 effective words) took 0.0s, 20463 effective words/s\n",
      "2021-01-21 19:40:56,684 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:40:56,685 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:40:56,685 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:40:56,686 : INFO : EPOCH - 3 : training on 140 raw words (46 effective words) took 0.0s, 29536 effective words/s\n",
      "2021-01-21 19:40:56,687 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:40:56,688 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:40:56,688 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:40:56,688 : INFO : EPOCH - 4 : training on 140 raw words (54 effective words) took 0.0s, 38529 effective words/s\n",
      "2021-01-21 19:40:56,691 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:40:56,692 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:40:56,694 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:40:56,695 : INFO : EPOCH - 5 : training on 140 raw words (51 effective words) took 0.0s, 12197 effective words/s\n",
      "2021-01-21 19:40:56,696 : INFO : training on a 700 raw words (250 effective words) took 0.0s, 11676 effective words/s\n",
      "2021-01-21 19:40:56,697 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2021-01-21 19:40:56,697 : INFO : precomputing L2-norms of word weight vectors\n",
      "2021-01-21 19:40:56,700 : WARNING : vectors for words {'las', 'gaj'} are not present in the model, ignoring these words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=79, size=100, alpha=0.025)\n",
      "<gensim.models.word2vec.Word2VecVocab object at 0x7f6ecdb8d890>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stwasik/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'harcerz'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sentences, iter=5, min_count=1)\n",
    "print(model)\n",
    "print(model.vocabulary)\n",
    "\n",
    "model.wv.doesnt_match(\"las harcerz gaj zioła\".split())\n",
    "\n",
    "# min_count oznacza minimalną liczbę wystąpień słowa, by było ono wzięte pod uwagę;\n",
    "# iter (epochs) to liczba iteracji po korpusie;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model jest niezwykle mały i niezbyt praktyczny, ale pozwolił pokazać podstawę uczenia word2vec. Przy większych korpusach tekstowych wczytywanie do pamięci wielkich tablic nie byłoby najlepszym pomysłem. Na szczęście implementacja word2vec w gensim potrafi przetwarzać dane przyrostowo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przetwarzanie strumieniowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zamiast wczytywać wszystkie dokumenty naraz można robić to partiami, bo sieci neuronowe (w tym word2vec) potrafią douczać się przyrostowo. Do douczania przyrostowego świetnie nada się pythonowy iterator lub generator. Jeśli nie kojarzysz na czym polega działanie iteratorów i generatorów, zobacz jak [wyjaśnia to Radim Rehurek](https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/).\n",
    "\n",
    "Zasymulujmy zdania/wersy/tweety przechowywane w osobnych plikach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smart_open, os\n",
    "\n",
    "if not os.path.exists('./data/'):\n",
    "    os.makedirs('./data/')\n",
    "\n",
    "filenames = ['./data/f' + str(i) +'.txt' for i in range(39)]\n",
    "\n",
    "if sentences is not None:\n",
    "    for i, fname in enumerate(filenames):\n",
    "        with smart_open.smart_open(fname, 'w') as fout:\n",
    "            for line in sentences[i]:\n",
    "                fout.write(line + ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 3: Mając powyższy zbiór dokumentów tekstowych, stwórz metodę która będzie \"leniwie\" iterowała przez zasymulowany zbiór danych. Podczas iterowania usuń znaki interpunkcyjne i zmień wszystkie litery na małe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-21 19:47:08,842 : INFO : collecting all words and their counts\n",
      "2021-01-21 19:47:08,844 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-01-21 19:47:08,847 : INFO : collected 66 word types from a corpus of 175 raw words and 35 sentences\n",
      "2021-01-21 19:47:08,848 : INFO : Loading a fresh vocabulary\n",
      "2021-01-21 19:47:08,848 : INFO : effective_min_count=1 retains 66 unique words (100% of original 66, drops 0)\n",
      "2021-01-21 19:47:08,849 : INFO : effective_min_count=1 leaves 175 word corpus (100% of original 175, drops 0)\n",
      "2021-01-21 19:47:08,850 : INFO : deleting the raw counts dictionary of 66 items\n",
      "2021-01-21 19:47:08,850 : INFO : sample=0.001 downsamples 66 most-common words\n",
      "2021-01-21 19:47:08,850 : INFO : downsampling leaves estimated 52 word corpus (29.8% of prior 175)\n",
      "2021-01-21 19:47:08,851 : INFO : estimated required memory for 66 words and 100 dimensions: 85800 bytes\n",
      "2021-01-21 19:47:08,851 : INFO : resetting layer weights\n",
      "2021-01-21 19:47:08,864 : INFO : training model with 3 workers on 66 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-01-21 19:47:08,870 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:47:08,870 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:47:08,871 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:47:08,871 : INFO : EPOCH - 1 : training on 175 raw words (52 effective words) took 0.0s, 8846 effective words/s\n",
      "2021-01-21 19:47:08,874 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:47:08,875 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:47:08,875 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:47:08,876 : INFO : EPOCH - 2 : training on 175 raw words (71 effective words) took 0.0s, 22053 effective words/s\n",
      "2021-01-21 19:47:08,879 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:47:08,879 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:47:08,880 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:47:08,880 : INFO : EPOCH - 3 : training on 175 raw words (54 effective words) took 0.0s, 17504 effective words/s\n",
      "2021-01-21 19:47:08,886 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:47:08,887 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:47:08,888 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:47:08,889 : INFO : EPOCH - 4 : training on 175 raw words (44 effective words) took 0.0s, 7008 effective words/s\n",
      "2021-01-21 19:47:08,892 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:47:08,893 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:47:08,894 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:47:08,894 : INFO : EPOCH - 5 : training on 175 raw words (54 effective words) took 0.0s, 15447 effective words/s\n",
      "2021-01-21 19:47:08,898 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:47:08,899 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:47:08,900 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:47:08,900 : INFO : EPOCH - 6 : training on 175 raw words (45 effective words) took 0.0s, 10102 effective words/s\n",
      "2021-01-21 19:47:08,904 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:47:08,905 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:47:08,906 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:47:08,907 : INFO : EPOCH - 7 : training on 175 raw words (49 effective words) took 0.0s, 10915 effective words/s\n",
      "2021-01-21 19:47:08,912 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:47:08,913 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:47:08,913 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:47:08,914 : INFO : EPOCH - 8 : training on 175 raw words (54 effective words) took 0.0s, 11586 effective words/s\n",
      "2021-01-21 19:47:08,919 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:47:08,920 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:47:08,920 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:47:08,921 : INFO : EPOCH - 9 : training on 175 raw words (61 effective words) took 0.0s, 11742 effective words/s\n",
      "2021-01-21 19:47:08,925 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:47:08,926 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:47:08,926 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:47:08,926 : INFO : EPOCH - 10 : training on 175 raw words (57 effective words) took 0.0s, 15208 effective words/s\n",
      "2021-01-21 19:47:08,927 : INFO : training on a 1750 raw words (541 effective words) took 0.1s, 8623 effective words/s\n",
      "2021-01-21 19:47:08,927 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2021-01-21 19:47:08,928 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=66, size=100, alpha=0.025)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'las'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            if fname.endswith('.txt'):\n",
    "                for line in open(os.path.join(self.dirname, fname)):\n",
    "                    line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "                    yield RE_SPACES.split(line.lower())\n",
    "\n",
    "# Do odkomentowania:\n",
    "sentences = MySentences('./data/')\n",
    "model = gensim.models.Word2Vec(sentences, iter=10, min_count=1)\n",
    "print(model)\n",
    "\n",
    "model.wv.doesnt_match(\"las harcerz gaj zioła\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trochę więcej danych i przykłady zastosowań\n",
    "\n",
    "Jak wspomniano wcześniej powyższa piosenka jest zbyt krótka by stworzyć przekonujący model podobieństwa między słowami. Przejdziemy teraz na język angielski i wykorzystamy korpus dołączony do biblioteki `gensim`. Ten korpus nie jest jeszcze duży, więc wyniki nie będą rewelacyjne. Potrzeba > 500 tys. słów, żeby oczekiwać rozsądnych wyników dla ogólnych zapytań, ale przy specjalistycznych zastosowaniach korpusy niekoniecznie muszą być takie duże.\n",
    "\n",
    "**Zad. 4: Korzystając ze zdobytej wiedzy na temat iteratorów, uzupełnij poniższy kod.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep\n",
    "lee_train_file = test_data_dir + 'lee_background.cor'\n",
    "\n",
    "class MyText(object):\n",
    "    def __iter__(self):\n",
    "        for line in open(lee_train_file):\n",
    "            # Załóż, że każda linia to dokument, zmień litery na małe,\n",
    "            # usuń podstawowe znaki interpunkcyjne i podziel według białych znaków\n",
    "            line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "            yield RE_SPACES.split(line.lower())\n",
    "\n",
    "sentences = MyText()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 5: Naucz model word2vec o rozmiarze 200, przez 100 epok, usuwając słowa występującerzadziej niż 5 razy. Wynik przypisz do zmiennej `model`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-21 19:49:18,851 : INFO : collecting all words and their counts\n",
      "2021-01-21 19:49:18,858 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-01-21 19:49:18,891 : INFO : collected 7586 word types from a corpus of 60146 raw words and 300 sentences\n",
      "2021-01-21 19:49:18,892 : INFO : Loading a fresh vocabulary\n",
      "2021-01-21 19:49:18,897 : INFO : effective_min_count=5 retains 1791 unique words (23% of original 7586, drops 5795)\n",
      "2021-01-21 19:49:18,898 : INFO : effective_min_count=5 leaves 50552 word corpus (84% of original 60146, drops 9594)\n",
      "2021-01-21 19:49:18,903 : INFO : deleting the raw counts dictionary of 7586 items\n",
      "2021-01-21 19:49:18,904 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2021-01-21 19:49:18,905 : INFO : downsampling leaves estimated 36364 word corpus (71.9% of prior 50552)\n",
      "2021-01-21 19:49:18,909 : INFO : estimated required memory for 1791 words and 200 dimensions: 3761100 bytes\n",
      "2021-01-21 19:49:18,910 : INFO : resetting layer weights\n",
      "2021-01-21 19:49:19,195 : INFO : training model with 3 workers on 1791 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-01-21 19:49:19,259 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:19,259 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:19,263 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:19,264 : INFO : EPOCH - 1 : training on 60146 raw words (36329 effective words) took 0.1s, 545676 effective words/s\n",
      "2021-01-21 19:49:19,318 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:19,319 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:19,322 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:19,323 : INFO : EPOCH - 2 : training on 60146 raw words (36358 effective words) took 0.1s, 630486 effective words/s\n",
      "2021-01-21 19:49:19,374 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:19,381 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:19,385 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:19,386 : INFO : EPOCH - 3 : training on 60146 raw words (36156 effective words) took 0.1s, 596597 effective words/s\n",
      "2021-01-21 19:49:19,444 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:19,449 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:19,450 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:19,451 : INFO : EPOCH - 4 : training on 60146 raw words (36359 effective words) took 0.1s, 577450 effective words/s\n",
      "2021-01-21 19:49:19,505 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:19,508 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:19,516 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:19,517 : INFO : EPOCH - 5 : training on 60146 raw words (36317 effective words) took 0.1s, 564975 effective words/s\n",
      "2021-01-21 19:49:19,573 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:19,574 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:19,578 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:19,579 : INFO : EPOCH - 6 : training on 60146 raw words (36480 effective words) took 0.1s, 610691 effective words/s\n",
      "2021-01-21 19:49:19,637 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:19,637 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:19,642 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:19,642 : INFO : EPOCH - 7 : training on 60146 raw words (36399 effective words) took 0.1s, 590319 effective words/s\n",
      "2021-01-21 19:49:19,701 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:19,702 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:19,704 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:19,704 : INFO : EPOCH - 8 : training on 60146 raw words (36348 effective words) took 0.1s, 602723 effective words/s\n",
      "2021-01-21 19:49:19,760 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:19,767 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:19,769 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:19,771 : INFO : EPOCH - 9 : training on 60146 raw words (36349 effective words) took 0.1s, 566244 effective words/s\n",
      "2021-01-21 19:49:19,823 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:19,823 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:19,832 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:19,833 : INFO : EPOCH - 10 : training on 60146 raw words (36422 effective words) took 0.1s, 603741 effective words/s\n",
      "2021-01-21 19:49:19,886 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:19,892 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:19,901 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:19,901 : INFO : EPOCH - 11 : training on 60146 raw words (36372 effective words) took 0.1s, 544814 effective words/s\n",
      "2021-01-21 19:49:19,962 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:19,965 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:19,978 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:19,979 : INFO : EPOCH - 12 : training on 60146 raw words (36430 effective words) took 0.1s, 480379 effective words/s\n",
      "2021-01-21 19:49:20,032 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:20,036 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:20,043 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:20,043 : INFO : EPOCH - 13 : training on 60146 raw words (36389 effective words) took 0.1s, 576714 effective words/s\n",
      "2021-01-21 19:49:20,109 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:20,114 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:20,119 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:20,120 : INFO : EPOCH - 14 : training on 60146 raw words (36410 effective words) took 0.1s, 483801 effective words/s\n",
      "2021-01-21 19:49:20,171 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:20,174 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:20,181 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:20,181 : INFO : EPOCH - 15 : training on 60146 raw words (36269 effective words) took 0.1s, 603060 effective words/s\n",
      "2021-01-21 19:49:20,238 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:20,239 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:20,243 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:20,244 : INFO : EPOCH - 16 : training on 60146 raw words (36387 effective words) took 0.1s, 597905 effective words/s\n",
      "2021-01-21 19:49:20,312 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:20,313 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:20,319 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:20,319 : INFO : EPOCH - 17 : training on 60146 raw words (36388 effective words) took 0.1s, 492023 effective words/s\n",
      "2021-01-21 19:49:20,388 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:20,389 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-21 19:49:20,391 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:20,392 : INFO : EPOCH - 18 : training on 60146 raw words (36330 effective words) took 0.1s, 514163 effective words/s\n",
      "2021-01-21 19:49:20,447 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:20,453 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:20,456 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:20,457 : INFO : EPOCH - 19 : training on 60146 raw words (36396 effective words) took 0.1s, 579759 effective words/s\n",
      "2021-01-21 19:49:20,512 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:20,522 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:20,524 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:20,525 : INFO : EPOCH - 20 : training on 60146 raw words (36436 effective words) took 0.1s, 552218 effective words/s\n",
      "2021-01-21 19:49:20,571 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:20,574 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:20,577 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:20,577 : INFO : EPOCH - 21 : training on 60146 raw words (36420 effective words) took 0.0s, 1135677 effective words/s\n",
      "2021-01-21 19:49:20,625 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:20,629 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:20,638 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:20,639 : INFO : EPOCH - 22 : training on 60146 raw words (36323 effective words) took 0.1s, 598200 effective words/s\n",
      "2021-01-21 19:49:20,703 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:20,704 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:20,710 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:20,712 : INFO : EPOCH - 23 : training on 60146 raw words (36413 effective words) took 0.1s, 510727 effective words/s\n",
      "2021-01-21 19:49:20,776 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:20,777 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:20,785 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:20,786 : INFO : EPOCH - 24 : training on 60146 raw words (36435 effective words) took 0.1s, 519695 effective words/s\n",
      "2021-01-21 19:49:20,848 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:20,853 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:20,854 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:20,855 : INFO : EPOCH - 25 : training on 60146 raw words (36463 effective words) took 0.1s, 540435 effective words/s\n",
      "2021-01-21 19:49:20,910 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:20,917 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:20,918 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:20,918 : INFO : EPOCH - 26 : training on 60146 raw words (36362 effective words) took 0.1s, 595494 effective words/s\n",
      "2021-01-21 19:49:20,965 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:20,972 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:20,974 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:20,975 : INFO : EPOCH - 27 : training on 60146 raw words (36421 effective words) took 0.1s, 655447 effective words/s\n",
      "2021-01-21 19:49:21,019 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:21,025 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:21,028 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:21,028 : INFO : EPOCH - 28 : training on 60146 raw words (36344 effective words) took 0.1s, 693251 effective words/s\n",
      "2021-01-21 19:49:21,079 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:21,086 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:21,088 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:21,089 : INFO : EPOCH - 29 : training on 60146 raw words (36376 effective words) took 0.1s, 618344 effective words/s\n",
      "2021-01-21 19:49:21,144 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:21,145 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:21,153 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:21,154 : INFO : EPOCH - 30 : training on 60146 raw words (36331 effective words) took 0.1s, 576473 effective words/s\n",
      "2021-01-21 19:49:21,210 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:21,218 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:21,221 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:21,222 : INFO : EPOCH - 31 : training on 60146 raw words (36466 effective words) took 0.1s, 553579 effective words/s\n",
      "2021-01-21 19:49:21,276 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:21,284 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:21,285 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:21,285 : INFO : EPOCH - 32 : training on 60146 raw words (36320 effective words) took 0.1s, 589515 effective words/s\n",
      "2021-01-21 19:49:21,343 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:21,345 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:21,349 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:21,350 : INFO : EPOCH - 33 : training on 60146 raw words (36285 effective words) took 0.1s, 579949 effective words/s\n",
      "2021-01-21 19:49:21,398 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:21,401 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:21,404 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:21,404 : INFO : EPOCH - 34 : training on 60146 raw words (36386 effective words) took 0.0s, 1115487 effective words/s\n",
      "2021-01-21 19:49:21,457 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:21,458 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:21,462 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:21,463 : INFO : EPOCH - 35 : training on 60146 raw words (36407 effective words) took 0.1s, 636382 effective words/s\n",
      "2021-01-21 19:49:21,526 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:21,527 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:21,532 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:21,533 : INFO : EPOCH - 36 : training on 60146 raw words (36452 effective words) took 0.1s, 531527 effective words/s\n",
      "2021-01-21 19:49:21,591 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:21,593 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:21,597 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:21,598 : INFO : EPOCH - 37 : training on 60146 raw words (36450 effective words) took 0.1s, 585134 effective words/s\n",
      "2021-01-21 19:49:21,656 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:21,662 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:21,665 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-21 19:49:21,666 : INFO : EPOCH - 38 : training on 60146 raw words (36408 effective words) took 0.1s, 551097 effective words/s\n",
      "2021-01-21 19:49:21,722 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:21,725 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:21,732 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:21,733 : INFO : EPOCH - 39 : training on 60146 raw words (36419 effective words) took 0.1s, 564932 effective words/s\n",
      "2021-01-21 19:49:21,779 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:21,783 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:21,789 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:21,790 : INFO : EPOCH - 40 : training on 60146 raw words (36344 effective words) took 0.1s, 659814 effective words/s\n",
      "2021-01-21 19:49:21,840 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:21,847 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:21,848 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:21,849 : INFO : EPOCH - 41 : training on 60146 raw words (36301 effective words) took 0.1s, 632039 effective words/s\n",
      "2021-01-21 19:49:21,898 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:21,904 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:21,907 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:21,907 : INFO : EPOCH - 42 : training on 60146 raw words (36280 effective words) took 0.1s, 643505 effective words/s\n",
      "2021-01-21 19:49:21,955 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:21,958 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:21,966 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:21,967 : INFO : EPOCH - 43 : training on 60146 raw words (36350 effective words) took 0.1s, 621455 effective words/s\n",
      "2021-01-21 19:49:22,010 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:22,012 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:22,019 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:22,019 : INFO : EPOCH - 44 : training on 60146 raw words (36474 effective words) took 0.1s, 709820 effective words/s\n",
      "2021-01-21 19:49:22,067 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:22,073 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:22,077 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:22,078 : INFO : EPOCH - 45 : training on 60146 raw words (36388 effective words) took 0.1s, 639289 effective words/s\n",
      "2021-01-21 19:49:22,131 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:22,134 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:22,135 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:22,136 : INFO : EPOCH - 46 : training on 60146 raw words (36437 effective words) took 0.1s, 638771 effective words/s\n",
      "2021-01-21 19:49:22,190 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:22,192 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:22,198 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:22,199 : INFO : EPOCH - 47 : training on 60146 raw words (36333 effective words) took 0.1s, 595893 effective words/s\n",
      "2021-01-21 19:49:22,245 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:22,246 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:22,254 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:22,255 : INFO : EPOCH - 48 : training on 60146 raw words (36345 effective words) took 0.1s, 668965 effective words/s\n",
      "2021-01-21 19:49:22,301 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:22,307 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:22,311 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:22,312 : INFO : EPOCH - 49 : training on 60146 raw words (36327 effective words) took 0.0s, 745024 effective words/s\n",
      "2021-01-21 19:49:22,360 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:22,369 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:22,371 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:22,372 : INFO : EPOCH - 50 : training on 60146 raw words (36392 effective words) took 0.1s, 619707 effective words/s\n",
      "2021-01-21 19:49:22,425 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:22,426 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:22,429 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:22,430 : INFO : EPOCH - 51 : training on 60146 raw words (36377 effective words) took 0.1s, 708046 effective words/s\n",
      "2021-01-21 19:49:22,483 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:22,493 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:22,495 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:22,496 : INFO : EPOCH - 52 : training on 60146 raw words (36293 effective words) took 0.1s, 557604 effective words/s\n",
      "2021-01-21 19:49:22,549 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:22,551 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:22,557 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:22,558 : INFO : EPOCH - 53 : training on 60146 raw words (36403 effective words) took 0.1s, 599963 effective words/s\n",
      "2021-01-21 19:49:22,615 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:22,616 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:22,623 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:22,624 : INFO : EPOCH - 54 : training on 60146 raw words (36375 effective words) took 0.1s, 564707 effective words/s\n",
      "2021-01-21 19:49:22,676 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:22,677 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:22,683 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:22,684 : INFO : EPOCH - 55 : training on 60146 raw words (36387 effective words) took 0.1s, 620158 effective words/s\n",
      "2021-01-21 19:49:22,729 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:22,733 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:22,735 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:22,736 : INFO : EPOCH - 56 : training on 60146 raw words (36403 effective words) took 0.1s, 722774 effective words/s\n",
      "2021-01-21 19:49:22,789 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:22,795 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:22,798 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:22,799 : INFO : EPOCH - 57 : training on 60146 raw words (36429 effective words) took 0.1s, 589038 effective words/s\n",
      "2021-01-21 19:49:22,842 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:22,847 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:22,851 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:22,852 : INFO : EPOCH - 58 : training on 60146 raw words (36363 effective words) took 0.1s, 710912 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-21 19:49:22,907 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:22,908 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:22,912 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:22,913 : INFO : EPOCH - 59 : training on 60146 raw words (36355 effective words) took 0.1s, 616014 effective words/s\n",
      "2021-01-21 19:49:22,962 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:22,966 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:22,971 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:22,972 : INFO : EPOCH - 60 : training on 60146 raw words (36491 effective words) took 0.1s, 640514 effective words/s\n",
      "2021-01-21 19:49:23,028 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:23,031 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:23,037 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:23,038 : INFO : EPOCH - 61 : training on 60146 raw words (36241 effective words) took 0.1s, 559253 effective words/s\n",
      "2021-01-21 19:49:23,083 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:23,087 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:23,090 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:23,091 : INFO : EPOCH - 62 : training on 60146 raw words (36377 effective words) took 0.1s, 712406 effective words/s\n",
      "2021-01-21 19:49:23,146 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:23,147 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:23,152 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:23,152 : INFO : EPOCH - 63 : training on 60146 raw words (36340 effective words) took 0.1s, 605281 effective words/s\n",
      "2021-01-21 19:49:23,197 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:23,201 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:23,202 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:23,203 : INFO : EPOCH - 64 : training on 60146 raw words (36388 effective words) took 0.0s, 738763 effective words/s\n",
      "2021-01-21 19:49:23,251 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:23,255 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:23,259 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:23,259 : INFO : EPOCH - 65 : training on 60146 raw words (36425 effective words) took 0.1s, 662240 effective words/s\n",
      "2021-01-21 19:49:23,309 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:23,312 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:23,318 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:23,319 : INFO : EPOCH - 66 : training on 60146 raw words (36280 effective words) took 0.1s, 623761 effective words/s\n",
      "2021-01-21 19:49:23,363 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:23,369 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:23,371 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:23,371 : INFO : EPOCH - 67 : training on 60146 raw words (36379 effective words) took 0.1s, 719192 effective words/s\n",
      "2021-01-21 19:49:23,416 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:23,420 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:23,422 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:23,422 : INFO : EPOCH - 68 : training on 60146 raw words (36379 effective words) took 0.0s, 729719 effective words/s\n",
      "2021-01-21 19:49:23,470 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:23,479 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:23,483 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:23,483 : INFO : EPOCH - 69 : training on 60146 raw words (36341 effective words) took 0.1s, 609793 effective words/s\n",
      "2021-01-21 19:49:23,539 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:23,547 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:23,550 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:23,550 : INFO : EPOCH - 70 : training on 60146 raw words (36384 effective words) took 0.1s, 552785 effective words/s\n",
      "2021-01-21 19:49:23,604 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:23,605 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:23,608 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:23,609 : INFO : EPOCH - 71 : training on 60146 raw words (36285 effective words) took 0.1s, 636280 effective words/s\n",
      "2021-01-21 19:49:23,653 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:23,655 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:23,659 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:23,660 : INFO : EPOCH - 72 : training on 60146 raw words (36440 effective words) took 0.0s, 743273 effective words/s\n",
      "2021-01-21 19:49:23,706 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:23,711 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:23,714 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:23,714 : INFO : EPOCH - 73 : training on 60146 raw words (36390 effective words) took 0.1s, 687681 effective words/s\n",
      "2021-01-21 19:49:23,765 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:23,770 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:23,774 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:23,774 : INFO : EPOCH - 74 : training on 60146 raw words (36278 effective words) took 0.1s, 621155 effective words/s\n",
      "2021-01-21 19:49:23,823 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:23,832 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:23,834 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:23,835 : INFO : EPOCH - 75 : training on 60146 raw words (36355 effective words) took 0.1s, 616333 effective words/s\n",
      "2021-01-21 19:49:23,886 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:23,895 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:23,898 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:23,899 : INFO : EPOCH - 76 : training on 60146 raw words (36359 effective words) took 0.1s, 583475 effective words/s\n",
      "2021-01-21 19:49:23,944 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:23,950 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:23,953 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:23,954 : INFO : EPOCH - 77 : training on 60146 raw words (36275 effective words) took 0.1s, 676927 effective words/s\n",
      "2021-01-21 19:49:24,005 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:24,014 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:24,017 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:24,018 : INFO : EPOCH - 78 : training on 60146 raw words (36476 effective words) took 0.1s, 591253 effective words/s\n",
      "2021-01-21 19:49:24,059 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-21 19:49:24,065 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:24,069 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:24,070 : INFO : EPOCH - 79 : training on 60146 raw words (36312 effective words) took 0.1s, 716738 effective words/s\n",
      "2021-01-21 19:49:24,126 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:24,127 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:24,133 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:24,133 : INFO : EPOCH - 80 : training on 60146 raw words (36374 effective words) took 0.1s, 585214 effective words/s\n",
      "2021-01-21 19:49:24,176 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:24,178 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:24,186 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:24,187 : INFO : EPOCH - 81 : training on 60146 raw words (36462 effective words) took 0.1s, 700894 effective words/s\n",
      "2021-01-21 19:49:24,231 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:24,232 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:24,239 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:24,240 : INFO : EPOCH - 82 : training on 60146 raw words (36326 effective words) took 0.1s, 711984 effective words/s\n",
      "2021-01-21 19:49:24,285 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:24,290 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:24,294 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:24,295 : INFO : EPOCH - 83 : training on 60146 raw words (36352 effective words) took 0.1s, 682040 effective words/s\n",
      "2021-01-21 19:49:24,346 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:24,353 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:24,356 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:24,357 : INFO : EPOCH - 84 : training on 60146 raw words (36374 effective words) took 0.1s, 597915 effective words/s\n",
      "2021-01-21 19:49:24,408 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:24,409 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:24,415 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:24,416 : INFO : EPOCH - 85 : training on 60146 raw words (36264 effective words) took 0.1s, 636697 effective words/s\n",
      "2021-01-21 19:49:24,461 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:24,465 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:24,468 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:24,469 : INFO : EPOCH - 86 : training on 60146 raw words (36199 effective words) took 0.1s, 701231 effective words/s\n",
      "2021-01-21 19:49:24,513 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:24,517 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:24,522 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:24,523 : INFO : EPOCH - 87 : training on 60146 raw words (36238 effective words) took 0.1s, 689756 effective words/s\n",
      "2021-01-21 19:49:24,569 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:24,574 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:24,576 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:24,577 : INFO : EPOCH - 88 : training on 60146 raw words (36309 effective words) took 0.1s, 683953 effective words/s\n",
      "2021-01-21 19:49:24,623 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:24,625 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:24,628 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:24,628 : INFO : EPOCH - 89 : training on 60146 raw words (36254 effective words) took 0.0s, 734148 effective words/s\n",
      "2021-01-21 19:49:24,679 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:24,681 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:24,688 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:24,689 : INFO : EPOCH - 90 : training on 60146 raw words (36239 effective words) took 0.1s, 615971 effective words/s\n",
      "2021-01-21 19:49:24,746 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:24,747 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:24,753 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:24,754 : INFO : EPOCH - 91 : training on 60146 raw words (36298 effective words) took 0.1s, 570841 effective words/s\n",
      "2021-01-21 19:49:24,808 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:24,808 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:24,817 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:24,817 : INFO : EPOCH - 92 : training on 60146 raw words (36434 effective words) took 0.1s, 589817 effective words/s\n",
      "2021-01-21 19:49:24,868 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:24,872 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:24,877 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:24,878 : INFO : EPOCH - 93 : training on 60146 raw words (36449 effective words) took 0.1s, 622575 effective words/s\n",
      "2021-01-21 19:49:24,926 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:24,933 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:24,936 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:24,936 : INFO : EPOCH - 94 : training on 60146 raw words (36487 effective words) took 0.1s, 642071 effective words/s\n",
      "2021-01-21 19:49:24,988 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:24,994 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:24,997 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:24,998 : INFO : EPOCH - 95 : training on 60146 raw words (36394 effective words) took 0.1s, 609739 effective words/s\n",
      "2021-01-21 19:49:25,046 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:25,047 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:25,055 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:25,056 : INFO : EPOCH - 96 : training on 60146 raw words (36288 effective words) took 0.1s, 642836 effective words/s\n",
      "2021-01-21 19:49:25,099 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:25,105 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:25,108 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:25,109 : INFO : EPOCH - 97 : training on 60146 raw words (36354 effective words) took 0.1s, 717840 effective words/s\n",
      "2021-01-21 19:49:25,155 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:25,157 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:25,164 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:25,165 : INFO : EPOCH - 98 : training on 60146 raw words (36401 effective words) took 0.1s, 668630 effective words/s\n",
      "2021-01-21 19:49:25,221 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:25,223 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-21 19:49:25,228 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:25,229 : INFO : EPOCH - 99 : training on 60146 raw words (36375 effective words) took 0.1s, 576735 effective words/s\n",
      "2021-01-21 19:49:25,274 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-21 19:49:25,275 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-21 19:49:25,278 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-21 19:49:25,278 : INFO : EPOCH - 100 : training on 60146 raw words (36356 effective words) took 0.0s, 757212 effective words/s\n",
      "2021-01-21 19:49:25,279 : INFO : training on a 6014600 raw words (3636583 effective words) took 6.1s, 597816 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=1791, size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sentences, size=200, iter=100, min_count=5)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 6: Odkomentuj poniższe linie i zobacz jak można wykorzystać uzyskany model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-21 19:49:26,607 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('stage', 0.48707613348960876)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['human', 'crime'], negative=['party'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-21 19:49:30,850 : WARNING : vectors for words {'lunch', 'input', 'cat'} are not present in the model, ignoring these words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"input is lunch he sentence cat\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.054812238\n",
      "0.2619623\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('human', 'tree'))\n",
    "print(model.wv.similarity('crime', 'murder'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uwagi dodatkowe:**\n",
    "+ uczenie modelu można zrównoleglić, ale trzeba doinstalować [Cythona](http://cython.org/)\n",
    "+ wytrenowany model można łatwo zapisać do pliku za pomocą: `model.save(path)`\n",
    "+ równie łatwo można go później wczytać: `model = gensim.models.Word2Vec.load(path)`\n",
    "+ ponieważ uczenie jest przyrostowe, można łatwo rozszerzyć istniejący słownik i douczyć model na nowych zdaniach:\n",
    "```\n",
    "model = gensim.models.Word2Vec.load(path)\n",
    "more_sentences = [['Advanced', 'users', 'can', 'load', 'a', 'model', 'and', 'continue', \n",
    "                  'training', 'it', 'with', 'more', 'sentences']]\n",
    "model.build_vocab(more_sentences, update=True)\n",
    "model.train(more_sentences, )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykorzystanie gotowego modelu do klasyfikacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Póki co sami trenowaliśmy word2vec i to na niedużych zbiorach danych. Na szczęście są już gotowe modele (przynajmniej dla języka angielskiego) nauczone na miliardach dokumentów i zawierające miliony słów. Przydatna lista takich modeli (wraz z kodem tworzącym usługę sieciową wykorzystującą model...) pod adresem: https://github.com/3Top/word2vec-api.\n",
    "\n",
    "**Zad. 7: Pobierz korpus Google News i zapisz pobrany plik do folderu data. Następnie wykonaj poniższy kod. Ta operacja zajmie jakieś 3-4 minuty i zużyje ok. 4 GB RAMU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-21 20:06:24,571 : INFO : loading projection weights from data/GoogleNews-vectors-negative300.bin.gz\n",
      "2021-01-21 20:07:12,724 : INFO : loaded (3000000, 300) matrix from data/GoogleNews-vectors-negative300.bin.gz\n",
      "2021-01-21 20:07:12,725 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.3 s, sys: 2.59 s, total: 49.9 s\n",
      "Wall time: 49.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"data/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 8: Zobacz jak działa model nauczony na tak dużym korpusie.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76640123\n",
      "0.32413524\n"
     ]
    }
   ],
   "source": [
    "print(wv.similarity('woman', 'man'))\n",
    "print(wv.similarity('woman', 'cat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, super. Mam świetny model, mogę nim podpowiadać słowa, wynajdować niepasujące elementy, uzupełniać zdania, znajdować synonimy, itd. Ale czy da się to jakoś wykorzystać do klasyfikacji? word2vec ma wektor na każde słowo - jak z tego zrobić wektor na ciąg słów?\n",
    "\n",
    "**Odpowiedź: można uśrednić znaczenie słów w dokuemncie poprzez zsumowanie wektorów wszystkich słów.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.vectors_norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        return np.zeros(wv.layer_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, review) for review in text_list ])\n",
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bardzo szybko spróbujemy zastosować to podejście do predykcji gatunku filmu na podstawie jego opisu. Poniżej kod wczytujący ciekawy zbiór danych oraz pokazujący jakie ma klasy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-21 20:11:59,988 : INFO : NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  movieId                                               plot  \\\n",
      "0           0        1  A little boy named Andy loves to be in his roo...   \n",
      "1           1        2  When two kids find and play a magical board ga...   \n",
      "2           2        3  Things don't seem to change much in Wabasha Co...   \n",
      "3           3        6  Hunters and their prey--Neil and his professio...   \n",
      "4           4        7  An ugly duckling having undergone a remarkable...   \n",
      "\n",
      "         tag  \n",
      "0  animation  \n",
      "1    fantasy  \n",
      "2     comedy  \n",
      "3     action  \n",
      "4    romance  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6ed87932d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df = pd.read_csv('data/tagged_plots_movielens.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "print(df.head())\n",
    "df.tag.value_counts().plot(kind=\"bar\", rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Szybko dzielimy dane na zbiór uczący i testowy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizujemy dane i wyliczamy reprezentację wektorową (za pomocą zsumowanych wektorów słów wrod2vec):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenized = test_data.apply(lambda r: w2v_tokenize_text(r['plot']), axis=1).values\n",
    "train_tokenized = train_data.apply(lambda r: w2v_tokenize_text(r['plot']), axis=1).values\n",
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uczymy i testujemy klasyfikator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stwasik/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "\n",
    "logreg = logreg.fit(X_train_word_average, train_data['tag'])\n",
    "predicted = logreg.predict(X_test_word_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patrzymy jak nam poszło:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trafność klasyfikacji 0.5349794238683128\n",
      "Macierz pomyłek\n",
      " [[24  2 10  0  1  5]\n",
      " [ 2  7 10  5  3  4]\n",
      " [ 2  4 60  3 14  3]\n",
      " [ 4  4  4  3  0  1]\n",
      " [ 3  0 12  1 17  2]\n",
      " [ 6  0  4  1  3 19]]\n"
     ]
    }
   ],
   "source": [
    "print('Trafność klasyfikacji %s' % accuracy_score(test_data.tag, predicted))\n",
    "cm = confusion_matrix(test_data.tag, predicted)\n",
    "print('Macierz pomyłek\\n %s' % cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak na brak porządnego przetwarzania wstępnego, nie jest to zły wynik. Mam nadzieję, że ten przykład pokazał jak można wykorzystać word2vec do tworzenia atrybutów dla problemów klasyfikacyjnych."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37664bitanaconda3virtualenv479a4a6d335c4a66b4edef7caa9238ba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
